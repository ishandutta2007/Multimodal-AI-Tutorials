# Chapter 3: Common Modalities in AI
Multimodal AI deals with various types of data, each offering unique information. The most common modalities include:

*   **Text:** Natural language in written form. This includes documents, articles, social media posts, captions, and speech transcripts. Text is rich in semantic information and is often used to provide context or describe other modalities.
*   **Images:** Static visual data, such as photographs, illustrations, and medical scans. Images convey spatial information, object presence, colors, textures, and scenes.
*   **Audio:** Sound data, including speech, music, environmental sounds, and animal vocalizations. Audio carries information about tone, emotion, identity, and events.
*   **Video:** A sequence of images (frames) combined with an audio track. Video is a highly complex modality as it encompasses both visual and temporal information, often with accompanying sound. It captures motion, interactions, and dynamic events.
*   **Speech:** A specific type of audio that involves human vocalizations. It's often treated as a distinct modality due to its linguistic content and specific processing techniques (e.g., speech recognition).
*   **Sensor Data:** Information collected from various sensors, such as LiDAR (for depth and distance), radar, accelerometers, gyroscopes, and physiological sensors (e.g., ECG, EEG). This data provides quantitative measurements about the physical world or biological states.
*   **Tabular Data:** Structured data organized in tables, often found in databases or spreadsheets. While seemingly simple, it can provide crucial contextual information when combined with other modalities.

Understanding the characteristics and challenges of each modality is crucial for effective multimodal system design.
